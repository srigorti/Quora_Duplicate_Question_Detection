{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, json\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.data_utils import get_file\n",
    "import datetime, time, json\n",
    "from time import time\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers import Flatten, Bidirectional, Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1 = []\n",
    "question2 = []\n",
    "is_duplicate = []\n",
    "is_dup=[]\n",
    "\n",
    "TRAIN_XL = 'Data/quora_duplicate_questions.xlsx'\n",
    "\n",
    "KERAS_DATASETS_DIR = os.getcwd()\n",
    "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300\n",
    "MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'\n",
    "HIDDEN = 50\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 100000\n",
    "NB_EPOCHS = 15\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question pairs1: 404290\n",
      "Question pairs2: 404290\n",
      "dup: 404290\n"
     ]
    }
   ],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "xl = pd.ExcelFile(TRAIN_XL)\n",
    "    \n",
    "df1 = xl.parse(\"quora_duplicate_questions\")\n",
    "q1 = df1['question1']\n",
    "q2 = df1['question2']\n",
    "is_duplicate = df1['is_duplicate']\n",
    "\n",
    "\n",
    "for ques in q1:\n",
    "    question1.append(text_to_word_list(ques))    \n",
    "for ques in q2:\n",
    "    question2.append(text_to_word_list(ques))\n",
    "    \n",
    "\n",
    "print('Question pairs1: %d' % len(question1))\n",
    "print('Question pairs2: %d' % len(question2))\n",
    "print('dup: %d' % len(is_duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290\n"
     ]
    }
   ],
   "source": [
    "print(len(is_duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = question1 + question2\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "if not exists(GLOVE_ZIP_FILE):\n",
    "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
    "    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
    "\n",
    "with open('glove.6B.300d.txt', encoding=\"utf8\" ) as f:\n",
    "    content = f.readlines()\n",
    "model = {}\n",
    "for line in content:\n",
    "    splitLine = line.split()\n",
    "    word = splitLine[0]\n",
    "    embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "    model[word] = embedding\n",
    "print (\"Done.\",len(model),\" words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 24781\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of question1 data tensor: (404290, 25)\n",
      "Shape of question2 data tensor: (404290, 25)\n",
      "Shape of label tensor: (404290,)\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = np.array(is_duplicate)\n",
    "labels = np.array(is_duplicate, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "85519\n",
      "[[ 0.         0.         0.        ...,  0.         0.         0.       ]\n",
      " [ 0.04656    0.21318   -0.0074364 ...,  0.0090611 -0.20989    0.053913 ]\n",
      " [-0.20017    0.14302    0.052055  ...,  0.034939  -0.12599    0.21863  ]\n",
      " ..., \n",
      " [-0.020654   0.051946  -0.19756   ..., -0.1902     0.27514    0.45159  ]\n",
      " [ 0.31079    0.5725     0.10701   ...,  0.14536    0.5736     0.59401  ]\n",
      " [-0.43546   -0.14073   -0.26553   ...,  0.42638   -0.03747    0.2603   ]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = min(len(Q1_train),len(Q2_train),\n",
    "                     len(Q1_test),len(Q2_test))\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "print(nb_words)\n",
    "print(word_embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model([<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "question11 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question22 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embed_layer = Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,trainable=False)\n",
    "\n",
    "encoded_q1 = embed_layer(question11)\n",
    "encoded_q2 = embed_layer(question22)\n",
    "\n",
    "\n",
    "lstm_layer = Bidirectional(LSTM(HIDDEN))\n",
    "\n",
    "question11_op = lstm_layer(encoded_q1)\n",
    "question22_op = lstm_layer(encoded_q2)\n",
    "\n",
    "malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([question11_op, question22_op])\n",
    "\n",
    "model = Model([question11,question22], output=[malstm_distance])\n",
    "\n",
    "\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 25, 300)      25656000    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100)          140400      embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 1)            0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,796,400\n",
      "Trainable params: 140,400\n",
      "Non-trainable params: 25,656,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/15\n",
      " - 634s - loss: 0.5080 - acc: 0.7730 - val_loss: 0.4721 - val_acc: 0.7946\n",
      "Epoch 2/15\n",
      " - 606s - loss: 0.4652 - acc: 0.8024 - val_loss: 0.4510 - val_acc: 0.8089\n",
      "Epoch 3/15\n",
      " - 521s - loss: 0.4443 - acc: 0.8154 - val_loss: 0.4427 - val_acc: 0.8127\n",
      "Epoch 4/15\n",
      " - 678s - loss: 0.4297 - acc: 0.8240 - val_loss: 0.4353 - val_acc: 0.8179\n",
      "Epoch 5/15\n",
      " - 614s - loss: 0.4182 - acc: 0.8292 - val_loss: 0.4306 - val_acc: 0.8224\n",
      "Epoch 6/15\n",
      " - 588s - loss: 0.4075 - acc: 0.8354 - val_loss: 0.4278 - val_acc: 0.8228\n",
      "Epoch 7/15\n",
      " - 536s - loss: 0.3994 - acc: 0.8402 - val_loss: 0.4247 - val_acc: 0.8250\n",
      "Epoch 8/15\n",
      " - 571s - loss: 0.3922 - acc: 0.8445 - val_loss: 0.4233 - val_acc: 0.8244\n",
      "Epoch 9/15\n",
      " - 570s - loss: 0.3856 - acc: 0.8480 - val_loss: 0.4236 - val_acc: 0.8275\n",
      "Epoch 10/15\n",
      " - 565s - loss: 0.3794 - acc: 0.8512 - val_loss: 0.4221 - val_acc: 0.8274\n",
      "Epoch 11/15\n",
      " - 565s - loss: 0.3742 - acc: 0.8540 - val_loss: 0.4208 - val_acc: 0.8293\n",
      "Epoch 12/15\n",
      " - 565s - loss: 0.3697 - acc: 0.8563 - val_loss: 0.4212 - val_acc: 0.8280\n",
      "Epoch 13/15\n",
      " - 577s - loss: 0.3653 - acc: 0.8590 - val_loss: 0.4211 - val_acc: 0.8308\n",
      "Epoch 14/15\n",
      " - 571s - loss: 0.3610 - acc: 0.8611 - val_loss: 0.4228 - val_acc: 0.8282\n",
      "Epoch 15/15\n",
      " - 564s - loss: 0.3571 - acc: 0.8639 - val_loss: 0.4199 - val_acc: 0.8303\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-da6cf0b42ec6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                     callbacks=callbacks)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training time finished.\\n{} epochs in {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtraining_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "training_start_time = time()\n",
    "\n",
    "malstm_trained = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    nb_epoch=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(NB_EPOCHS, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy at epoch 13 = 0.8308\n"
     ]
    }
   ],
   "source": [
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_trained.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4361, accuracy = 0.8239\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
